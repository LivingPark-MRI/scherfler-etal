{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Study to replicate\n",
    "\n",
    "This notebook identifies PPMI subjects to reproduce the following paper:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Scherfler, Christoph, et al. <a href=https://onlinelibrary.wiley.com/doi/pdf/10.1002/ana.22245>White and gray matter abnormalities in idiopathic rapid eye movement sleep behavior disorder: a diffusion‐tensor imaging and voxel‐based morphometry study.</a> Annals of neurology 69.2 (2011): 400-407. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This study recruited 34 patients with iRBD and confirmed iRBD diagnosis with polysomnography. Patients had no PD or dementia at time of MRI. The demographics parameters were as follows (table extracted from the paper):\n",
    "\n",
    "<img src=\"images/demographics.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main results of the paper regarding structural MRI is that Voxel-Based Morphometry (VBM) reveals an increase of gray matter density in both hippocampi of iRBD patients, as illustrated in the following table extracted from the paper (diffusion MRI results are truncated):\n",
    "\n",
    "<img src=\"images/results.png\" width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remainder of this notebook is an attempt to replicate this result using the PPMI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LivingPark notebooks use a *cache* directory to store analysis inputs and outputs. Inputs typically include PPMI Study Data and imaging data whereas outputs include processed images and other derivatives. Cache directories allow LivingPark notebooks to run in a few minutes as they reuse previously computed results. However, cache directories cannot be made public due to the PPMI Data Usage Agreement (DUA). Instead, they are stored on `login.bic.mni.mcgill.ca`, which requires a specific user name and password as well as a properly-configured DataLad installation in your account (see instructions [here](https://docs.google.com/document/d/1K7RzQjYC1O6tpSqe_c0CvxvJq38J10zNrDIdw20Yh_0/edit?usp=sharing)). In case you don't have access to the cache directory of this notebook, the next sections will download all the required imaging data from PPMI and recompute the results, which will take a few hours depending on your computer configuration. In the future, we will aim at storing this cache dataset on PPMI servers so that they can be accessed with a PPMI account.\n",
    "\n",
    "Let's initialize the cache directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing notebook dependencies (see log in install.log)... \n",
      "This notebook was run on 2022-07-15 15:42:33.692355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>\n",
       "            code_show=true; \n",
       "            function code_toggle() {\n",
       "                 if (code_show){\n",
       "                 $('div.input').hide();\n",
       "                 } else {\n",
       "                 $('div.input').show();\n",
       "                 }\n",
       "                 code_show = !code_show\n",
       "            } \n",
       "            $( document ).ready(code_toggle);\n",
       "            </script>\n",
       "            <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the Python code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import livingpark_utils\n",
    "\n",
    "utils = livingpark_utils.LivingParkUtils(\"scherfler-etal\")\n",
    "utils.prologue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPMI cohort preparation\n",
    "\n",
    "We will build a PPMI cohort that matches the one used in the original study (Table 1) as much as possible. As in other LivingPark replications, we will use the same sample size as the original study. Our cohort will be built directly from PPMI Study Data files so that it can be replicated and updated whenever necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Study data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will start by downloading the PPMI Study Data files required to build our cohort: \n",
    "* Age at visit (this could also be retrieved from imaging data)\n",
    "* Demographics (to retrieve sex)\n",
    "* REM Sleep Behavior Disorder Questionnaire\n",
    "\n",
    "We will use the LivingPark utils library to download these files from the notebook. If files are already present in the notebook cache, they won't be downloaded again. Otherwise, you will need to enter your PPMI username and password. In case you don't have a PPMI account, you can request one [here](http://ppmi-info.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files are now available: ['Age_at_visit.csv', 'Demographics.csv', 'REM_Sleep_Behavior_Disorder_Questionnaire.csv']\n"
     ]
    }
   ],
   "source": [
    "required_files = [\n",
    "    \"Age_at_visit.csv\",\n",
    "    \"Demographics.csv\",\n",
    "    \"REM_Sleep_Behavior_Disorder_Questionnaire.csv\",\n",
    "]\n",
    "\n",
    "utils.install_ppmi_study_files(required_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need file `MRI_info.csv` produced by another LivingPark notebook available at https://github.com/LivingPark-MRI/ppmi-MRI-metadata. This file contains a list of T1-weighted MRIs usable for VBM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File inputs/study_files/MRI_info.csv is now available\n"
     ]
    }
   ],
   "source": [
    "# TODO: move this to livingpark_utils\n",
    "import os.path as op\n",
    "\n",
    "file_path = op.join(utils.study_files_dir, \"MRI_info.csv\")\n",
    "if not op.exists(file_path):\n",
    "    !(cd {utils.study_files_dir} && python -m wget \"https://raw.githubusercontent.com/LivingPark-MRI/ppmi-MRI-metadata/main/MRI metadata.ipynb\")  # use requests to improve portability\n",
    "    npath = op.join(utils.study_files_dir, \"MRI metadata.ipynb\")\n",
    "    %run \"{npath}\"\n",
    "print(f\"File {file_path} is now available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## RBD Score computation\n",
    "\n",
    "The original study used polysomnography to diagnose REM Sleep Behavior Disorder, which is not available in PPMI.\n",
    "Instead, we will use the REM Sleep Behavior Disorder Screening Questionnaire (RSBDSQ) [[1]](https://movementdisorders.onlinelibrary.wiley.com/doi/10.1002/mds.21740) to discriminate RBD patients from controls. This questionnaire consists of 13 yes/no questions that are summed to produce an RBD score. We computed the RBD score by summing the answers to each of the 13 questions. The answers to questions Q1 to Q9 (12 questions as Q6 is sub-divided in 4 parts) are directly availble in the RSBDSQ Study Data file. We computed the answer to Q10 (\"I have/had a disease of the nervous system (e.g., stroke, head trauma, parkinsonism, RLS, narcolepsy, depression, epilepsy, inflammatory disease of the brain), which?\") by taking the max of the 8 corresponding disease variables in the RSBDSQ Study Data file. We also computed the answer to Q6 as the sum of Q6.1 to Q6.4 as it will use as inclusion criterion for controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following distribution of RBD scores in the PPMI dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load PPMI RSBDQ data\n",
    "df = pd.read_csv(\n",
    "    op.join(utils.study_files_dir, \"REM_Sleep_Behavior_Disorder_Questionnaire.csv\")\n",
    ")\n",
    "\n",
    "# Compute RBDSQ score\n",
    "df[\"RBDSQ\"] = (\n",
    "    df[\"DRMVIVID\"]  # Q1\n",
    "    + df[\"DRMAGRAC\"]  # Q2\n",
    "    + df[\"DRMNOCTB\"]  # Q3\n",
    "    + df[\"SLPLMBMV\"]  # Q4\n",
    "    + df[\"SLPINJUR\"]  # Q5\n",
    "    + df[\"DRMVERBL\"]  # Q6.1\n",
    "    + df[\"DRMFIGHT\"]  # Q6.2\n",
    "    + df[\"DRMUMV\"]  # Q6.3\n",
    "    + df[\"DRMOBJFL\"]  # Q6.4\n",
    "    + df[\"MVAWAKEN\"]  # Q7\n",
    "    + df[\"DRMREMEM\"]  # Q8\n",
    "    + df[\"SLPDSTRB\"]  # Q9\n",
    "    + df[  # Q10\n",
    "        [\n",
    "            \"BRNINFM\",\n",
    "            \"DEPRS\",\n",
    "            \"EPILEPSY\",\n",
    "            \"HETRA\",\n",
    "            \"NARCLPSY\",\n",
    "            \"PARKISM\",\n",
    "            \"RLS\",\n",
    "            \"STROKE\",\n",
    "        ]\n",
    "    ].max(axis=1)\n",
    ")\n",
    "\n",
    "df[\"Q6\"] = df[\"DRMVERBL\"] + df[\"DRMFIGHT\"] + df[\"DRMUMV\"] + df[\"DRMOBJFL\"]\n",
    "\n",
    "# Note: CNSOTHCM isn't present in data\n",
    "\n",
    "# Check that max RBDSQ score is <= 13\n",
    "assert df[\"RBDSQ\"].max() <= 13\n",
    "\n",
    "df.groupby(\"RBDSQ\").count()[\"REC_ID\"].plot.bar()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.ylabel(\"Number of records\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inclusion criteria\n",
    "\n",
    "Consistently with the original study, we only include subjects with no Parkinsonism. For these subjects, we include all the visits that have a T1-weighted MRI usable for VBM (see [MRI metadata notebook](https://github.com/LivingPark-MRI/ppmi-MRI-metadata)).  We include subjects in the RBD group when their RBD score is >= 5. We include subjects in the control group when their RBD score is < 5 and their score to Q6 of the RSBDSQ is 0 [**TODO** ref needed, see email thread between Madeleine and Ron Postuma]. In case multiple visits of a given subject are included in the RBD of Control group, we randomly select one of them for inclusion in the group and exclude the other ones. This selection ensures that a given subject is included at most once in the RBD group and at most once in the control group. In Section \"Cohort matching\", we will ensure that a given subjet is included in at most one group. \n",
    "\n",
    "We obtain the following group sizes:\n",
    "\n",
    "<!-- and a cutoff score of 6 to identify RBD subjects among PD subjects, consistently with the results presented in [[2]](https://www.sciencedirect.com# /science/article/pii/S138994571100164X). -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RBD subjects: 415\n",
      "Number of control subjects: 580\n"
     ]
    }
   ],
   "source": [
    "# Read demographics and MRI data.\n",
    "mri = pd.read_csv(op.join(utils.study_files_dir, \"MRI_info.csv\"))[\n",
    "    [\"Subject ID\", \"Visit code\", \"Description\"]\n",
    "]\n",
    "mri.rename(columns={\"Subject ID\": \"PATNO\", \"Visit code\": \"EVENT_ID\"}, inplace=True)\n",
    "dem = pd.read_csv(op.join(\"data\", \"Demographics.csv\"))[[\"PATNO\", \"SEX\"]]\n",
    "age = pd.read_csv(op.join(\"data\", \"Age_at_visit.csv\"))\n",
    "\n",
    "# Merge RBD data with demographics, age and MRI\n",
    "merged = (\n",
    "    df.merge(dem, on=\"PATNO\", how=\"inner\")\n",
    "    .merge(age, on=[\"PATNO\", \"EVENT_ID\"], how=\"inner\")\n",
    "    .merge(mri, on=[\"PATNO\", \"EVENT_ID\"], how=\"inner\")[\n",
    "        [\"PATNO\", \"EVENT_ID\", \"RBDSQ\", \"Q6\", \"SEX\", \"AGE_AT_VISIT\", \"Description\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract RBD subjects and controls according to inclusion criteria\n",
    "# Sample one entry (event and T1 description) per subject in each group so that a subject happens at most once in a group (between-group consistency will be ensured in cohort matching)\n",
    "# Sort entries by PATNO to improve reproducibility of this notebook\n",
    "rbds = (\n",
    "    merged[merged[\"RBDSQ\"] >= 5]\n",
    "    .groupby(\"PATNO\")\n",
    "    .sample(1, random_state=1)\n",
    "    .sort_values(by=\"PATNO\")\n",
    ")\n",
    "controls = (\n",
    "    merged[(merged[\"RBDSQ\"] < 5) & (merged[\"Q6\"] == 0)]\n",
    "    .groupby(\"PATNO\")\n",
    "    .sample(1, random_state=1)\n",
    "    .sort_values(by=\"PATNO\")\n",
    ")\n",
    "\n",
    "print(f\"Number of RBD subjects: {len(rbds)}\")\n",
    "print(f\"Number of control subjects: {len(controls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cohort matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The RBD and Control groups built previously give us a lot of flexibility to build a cohort that matches the one in the original study, due to their large sample size compared to the original cohort. We adopted the following approach to construct a matching cohort:\n",
    "1. Randomly select 4 control women and 10 control men, to reproduce the F/M balance in the original study\n",
    "2. Find 26 subjects from the RBD group that best match age and sex in the selected control group. We matched sex by direct sampling of males and females using the same proportion as in the original study. We matched age using a nearest-neighbor approach.\n",
    "\n",
    "We obtain the following cohort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nn_match(sample1, df_2, n2, cat_variables, num_variables, random_state=0):\n",
    "    \"\"\"\n",
    "    Find len(sample1) rows in df_2 such that variables are matched with sample1.\n",
    "\n",
    "    sample1: samples in group1\n",
    "    df_2: dataframe with subjects in group 2\n",
    "    n2: desired sample size for group 2\n",
    "    cat_variables: categorical variables to match\n",
    "    num_variables: numerical variables to match\n",
    "    \"\"\"\n",
    "\n",
    "    def nn(x, df, variables):\n",
    "        \"\"\"\n",
    "        Find index of nearest neighbor of x in df\n",
    "\n",
    "        * x: a dataframe row\n",
    "        * df: a dataframe\n",
    "        * variables: variables to match. Should be normalized.\n",
    "        \"\"\"\n",
    "        df[\"dist\"] = sum((df[var] - x[var]) ** 2 for var in variables)\n",
    "        df.sort_values(\"dist\", inplace=True)\n",
    "        return df.head(1).index[\n",
    "            0\n",
    "        ]  ## there's probably a better way to do it but it works\n",
    "\n",
    "    # Check assumptions\n",
    "    n1 = len(sample1)\n",
    "    assert n1 <= n2\n",
    "    for v in num_variables + cat_variables:\n",
    "        assert v in sample1 and v in df_2\n",
    "\n",
    "    # Copy original dataframe to leave them untouched\n",
    "    df_2_ = df_2.copy()\n",
    "    sample1_ = sample1.copy()\n",
    "\n",
    "    # Normalize variables to match to compute meaningful distances\n",
    "    for v in num_variables:\n",
    "        m = df_2_[v].mean()\n",
    "        s = df_2_[v].std()\n",
    "        for df in (df_2_, sample1_):\n",
    "            df[v] = (df[v] - m) / s\n",
    "\n",
    "    # For each subject in sampled group 1,\n",
    "    # find one or more subject in sampled group 2, without replacement.\n",
    "    indices = []\n",
    "    for i in range(n2):\n",
    "        j = i % n1  # loop over sample1\n",
    "        df_2_cat = df_2_.copy()\n",
    "        for c in cat_variables:\n",
    "            df_2_cat = df_2_cat[df_2_cat[c] == sample1_.iloc[j][c]]\n",
    "        assert len(df_2_cat) > 0\n",
    "        index = nn(sample1_.iloc[j], df_2_cat, num_variables)\n",
    "        df_2_.drop(index=index, inplace=True)\n",
    "        indices.append(index)\n",
    "\n",
    "    sample2 = df_2[df_2.index.isin(indices)]\n",
    "\n",
    "    return sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Randomly select 4 control women and 10 control men, to reproduce F/M balance in original paper\n",
    "controls = pd.concat(\n",
    "    [\n",
    "        controls[controls[\"SEX\"] == 0].sample(n=4, random_state=1),\n",
    "        controls[controls[\"SEX\"] == 1].sample(n=10, random_state=1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Match with RBD subjects\n",
    "rbds = nn_match(controls, rbds, 26, [\"SEX\"], [\"AGE_AT_VISIT\"], random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t| iRBD Patients\t| Controls\n",
      "Subjects, No. \t\t| 26 \t\t| 14\n",
      "F/M, No. \t\t| 8/18 \t\t| 4/10\n",
      "Age, mean +/- SD \t| 59.1 +/- 10.6 \t| 59.7 +/- 10.7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\n",
    "    \"\\t\\t\\t| iRBD Patients\\t| Controls\"\n",
    "    + os.linesep\n",
    "    + f\"Subjects, No. \\t\\t| {len(rbds)} \\t\\t| {len(controls)}\"\n",
    "    + os.linesep\n",
    "    + f\"F/M, No. \\t\\t| {len(rbds[rbds['SEX']==0])}/{len(rbds[rbds['SEX']==1])} \\t\\t| {len(controls[controls['SEX']==0])}/{len(controls[controls['SEX']==1])}\"\n",
    "    + os.linesep\n",
    "    + f\"Age, mean +/- SD \\t| {round(rbds['AGE_AT_VISIT'].mean(),1)} +/- {round(rbds['AGE_AT_VISIT'].std(),1)} \\t| {round(controls['AGE_AT_VISIT'].mean(),1)} +/- {round(controls['AGE_AT_VISIT'].std(),1)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The demographics parameters of the selected PPMI subjects seem comparable to the ones in the initial study. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do basic sanity checks in this cohort:\n",
    "* A subject appears at most once in each group\n",
    "* A subject does not appear in more than one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sanity checks passed.\n"
     ]
    }
   ],
   "source": [
    "assert len(pd.unique(rbds[\"PATNO\"])) == len(\n",
    "    rbds\n",
    "), \"Some subjects are present more than once in RBD group\"\n",
    "assert len(pd.unique(controls[\"PATNO\"])) == len(\n",
    "    controls\n",
    "), \"Some subjects are present more than once in controls\"\n",
    "assert (\n",
    "    len(set(controls[\"PATNO\"]) & set(rbds[\"PATNO\"])) == 0\n",
    "), \"Some subjects are present in controls and rbd group\"\n",
    "print(\"All sanity checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate both groups in cohort DataFrame\n",
    "rbds[\"group\"] = \"RBD\"\n",
    "controls[\"group\"] = \"Control\"\n",
    "cohort = pd.concat([rbds, controls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we generate a cohort id that uniquely identifies the cohort built previously. We will use this id to avoid recomputing the same results multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort id: _3913746513060334940\n"
     ]
    }
   ],
   "source": [
    "cohort_id = str(hash(tuple(sorted(cohort[\"PATNO\"])))).replace(\n",
    "    \"-\", \"_\"\n",
    ")  # - in filenames crash SPM (Matlab)\n",
    "print(f\"Cohort id: {cohort_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort patient ids saved in scherfler-etal-patnos-_3913746513060334940.csv. Do not share this file publicly!\n"
     ]
    }
   ],
   "source": [
    "# Save patient ids\n",
    "filename = f\"scherfler-etal-patnos-{cohort_id}.csv\"\n",
    "cohort[\"PATNO\"].to_csv(filename, index=False)\n",
    "print(f\"Cohort patient ids saved in {filename}. Do not share this file publicly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image analysis\n",
    "\n",
    "Structural MRI analysis in the original paper is a straightforward VBM analysis implemented with SPM using the DARTEL toolbox. To replicate it, we mostly followed the excellent tutorial on VBM in SPM available at https://www.fil.ion.ucl.ac.uk/~john/misc/VBMclass15.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first check if the Nifti image files associated with the subjects and visits selected in our cohort are available in cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available subjects: 40\n",
      "Number of missing subjects: 0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "cohort[\"File name\"] = cohort.apply(\n",
    "    lambda x: utils.find_nifti_file_in_cache(\n",
    "        x[\"PATNO\"], x[\"EVENT_ID\"], x[\"Description\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "print(f\"Number of available subjects: {len(cohort[cohort['File name'].notna()])}\")\n",
    "print(f\"Number of missing subjects: {len(cohort[cohort['File name'].isna()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now download the missing image files and move them to the data cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image data of 0 subjects\n"
     ]
    }
   ],
   "source": [
    "from ppmi_downloader import PPMIDownloader\n",
    "\n",
    "ppmi_dl = PPMIDownloader()\n",
    "\n",
    "missing_subject_ids = cohort[cohort[\"File name\"].isna()][\"PATNO\"]\n",
    "print(f\"Downloading image data of {len(missing_subject_ids)} subjects\")\n",
    "ppmi_dl.download_imaging_data(\n",
    "    missing_subject_ids,\n",
    "    type=\"nifti\",\n",
    "    timeout=120 * len(missing_subject_ids),\n",
    "    headless=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppmi_downloader import PPMINiftiFileFinder\n",
    "import os\n",
    "\n",
    "results_path = \"outputs\"\n",
    "\n",
    "ppmi_fd = PPMINiftiFileFinder()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "for _, row in cohort.iterrows():\n",
    "    if row[\"File name\"] is None:\n",
    "        filename = ppmi_fd.find_nifti(row[\"PATNO\"], row[\"EVENT_ID\"], row[\"Description\"])\n",
    "        if filename is None:\n",
    "            print(f\"Not found: {row['PATNO'], row['EVENT_ID'], row['Description']}\")\n",
    "        else:  # copy file to dataset\n",
    "            dest_dir = op.join(\n",
    "                dataset_name, f'sub-{row[\"PATNO\"]}', f'ses-{row[\"EVENT_ID\"]}', \"anat\"\n",
    "            )\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            os.rename(filename, op.join(dest_dir, op.basename(filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort[\"File name\"] = cohort.apply(\n",
    "    lambda x: utils.find_nifti_file_in_cache(\n",
    "        x[\"PATNO\"], x[\"EVENT_ID\"], x[\"Description\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link input file to results dir because SPM writes next to input files\n",
    "for file_name in cohort[\"File name\"].values:\n",
    "    dest_dir = op.dirname(file_name).replace(\n",
    "        op.join(utils.data_cache_path, \"inputs\"),\n",
    "        op.join(results_path, \"pre_processing\"),\n",
    "    )\n",
    "    dest_file = op.join(dest_dir, op.basename(file_name))\n",
    "    if not op.exists(dest_file):\n",
    "        os.makedirs(dest_dir, exist_ok=True)\n",
    "        os.symlink(op.relpath(op.abspath(file_name), start=dest_dir), dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing consists of:\n",
    "1. Data pre-processing: segmentation, DARTEL template generation, normalization to MNI space\n",
    "2. Intra-cranial volume computation (to be used as covariates in the statistical model)\n",
    "3. Statistical model\n",
    "\n",
    "We implemented each of these steps as an SPM batch. For each batch, we created a template where file names will be inserted. Batch templates are available in `code/templates`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the default parameters in SPM Segment, DARTEL, and Normalize to MNI Space modules, except for the Gaussian smoothing kernel size (FWHM) in the Normalise module that we set to 4 mm $\\times$ 4 mm $\\times$ 4 mm consistently with the original study. Our pre-processing batch template is in `code/templates/pre_processing_job.m`.\n",
    "\n",
    "We will now add our cohort subjects to the pre-processing batch template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import datalad.api as dat\n",
    "\n",
    "\n",
    "def write_batch_files(batch_job_filename, replaced_keys, tempfile_name):\n",
    "    \"\"\"\n",
    "    tempfile_name: should end in _job.m\n",
    "    \"\"\"\n",
    "\n",
    "    def replace_keys(string, replace_keys):\n",
    "        for k in replace_keys:\n",
    "            string = string.replace(k, replace_keys[k])\n",
    "        return string\n",
    "\n",
    "    # Batch job file\n",
    "    with open(batch_job_filename, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    assert tempfile_name.endswith(\"_job.m\")\n",
    "\n",
    "    with open(tempfile_name, \"w\") as f:\n",
    "        f.write(replace_keys(content, replaced_keys))\n",
    "\n",
    "    print(f\"Job batch file written in {op.basename(tempfile_name)}\")\n",
    "\n",
    "    # Batch file\n",
    "    batch_file = op.join(\"code\", \"templates\", \"call_batch.m\")\n",
    "    tempfile_name_batch = tempfile_name.replace(\"_job\", \"_batch\")\n",
    "\n",
    "    with open(batch_file, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    with open(tempfile_name_batch, \"w\") as f:\n",
    "        f.write(\n",
    "            replace_keys(\n",
    "                content,\n",
    "                {\n",
    "                    \"[BATCH]\": f\"addpath('{op.join(os.getcwd(), 'code', 'batches')}')\"\n",
    "                    + os.linesep\n",
    "                    + op.basename(tempfile_name.replace(\".m\", \"\"))\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(f\"Batch file written in {op.basename(tempfile_name_batch)}\")\n",
    "\n",
    "\n",
    "def run_batch_file(job_filename):\n",
    "\n",
    "    log_file_name = op.abspath(\n",
    "        op.join(\n",
    "            results_path, \"logs\", op.basename(job_filename.replace(\"_job.m\", \".log\"))\n",
    "        )\n",
    "    )\n",
    "    spm_batch_file = job_filename.replace(\"_job\", \"_batch\")\n",
    "\n",
    "    if op.exists(log_file_name):\n",
    "        print(\n",
    "            f\"Log file {op.basename(log_file_name)} exists, skipping batch execution (remove file to force execution)\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    print(f\"Log file {op.basename(log_file_name)} does not exist, running batch\")\n",
    "\n",
    "    output = spm_batch(\n",
    "        \"launch\", \"-s\", \"-u\", spm_batch_file=spm_batch_file, log_file_name=log_file_name\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        output.exit_code == 0\n",
    "    ), f\"Execution error, inspect output object for logs: {output}\"\n",
    "\n",
    "    print(f\"Execution was successful.\")\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job batch file written in pre_processing__3913746513060334940_job.m\n",
      "Batch file written in pre_processing__3913746513060334940_batch.m\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Preprocessing batch\n",
    "preprocessing_batch_job = op.join(\"code\", \"templates\", \"pre_processing_job.m\")\n",
    "preprocessing_name = op.abspath(\n",
    "    op.join(\"code\", \"batches\", f\"pre_processing_{cohort_id}_job.m\")\n",
    ")\n",
    "\n",
    "image_files = sorted(\n",
    "    [\n",
    "        op.abspath(\n",
    "            utils.find_nifti_file_in_cache(\n",
    "                row[\"PATNO\"],\n",
    "                row[\"EVENT_ID\"],\n",
    "                row[\"Description\"],\n",
    "                base_dir=op.join(\"outputs\", \"pre_processing\"),\n",
    "            )\n",
    "        )\n",
    "        for index, row in cohort.iterrows()\n",
    "    ]\n",
    ")\n",
    "image_files_quote = [f\"'{x},1'\" for x in image_files]\n",
    "write_batch_files(\n",
    "    preprocessing_batch_job,\n",
    "    {\"[IMAGES]\": os.linesep.join(image_files_quote)},\n",
    "    tempfile_name=preprocessing_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the pre-processing batch. To ensure that it will run on this computer, we containerized it with Docker and wrapped it in a Boutiques descriptor. Let's run the batch. If it's the first time that you run this notebook, the Docker container will be downloaded to your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Unlocking files \n",
      "[INFO] Completed unlocking files \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dat.Dataset(utils.data_cache_path)\n",
    "\n",
    "dat.unlock(\n",
    "    dataset=d\n",
    ")  # unlock the dataset so that results can be overwritten if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory creations\n",
    "\n",
    "for d in [\"logs\", \"results\"]:\n",
    "    os.makedirs(op.join(results_path, d), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file pre_processing__3913746513060334940.log exists, skipping batch execution (remove file to force execution)\n"
     ]
    }
   ],
   "source": [
    "# Boutiques initialization for SPM batch\n",
    "from boutiques.descriptor2func import function\n",
    "\n",
    "spm_batch = function(\n",
    "    op.join(\"envs\", \"docker\", \"spm-batch.json\")\n",
    ")  # Use Octave 5.2.0 for DARTEL or it crashes\n",
    "\n",
    "output = run_batch_file(preprocessing_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-cranial volume computation\n",
    "\n",
    "We compute intra-cranial volumes by (1) computing tissue volumes using the corresponding tool in SPM, (2) summing the grey-matter, white-matter, and CSF volumes. \n",
    "\n",
    "First, let's run the tissue volumes batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job batch file written in tissue_volumes__3913746513060334940_job.m\n",
      "Batch file written in tissue_volumes__3913746513060334940_batch.m\n",
      "Log file tissue_volumes__3913746513060334940.log exists, skipping batch execution (remove file to force execution)\n"
     ]
    }
   ],
   "source": [
    "# Tissue volumes batch\n",
    "tissue_volumes_batch_job = op.join(\"code\", \"templates\", \"tissue_volumes_job.m\")\n",
    "volumes_name = preprocessing_name.replace(\"pre_processing\", \"tissue_volumes\")\n",
    "\n",
    "segmentation_files = sorted(\n",
    "    [x.replace(\".nii\", \"_seg8.mat\").replace(\",1\", \"\") for x in image_files]\n",
    ")\n",
    "volumes_file = op.abspath(\n",
    "    op.join(results_path, op.basename(volumes_name.replace(\".m\", \".txt\")))\n",
    ")  # output file containing brain volumes\n",
    "write_batch_files(\n",
    "    tissue_volumes_batch_job,\n",
    "    {\n",
    "        \"[SEGMENTATION_FILES]\": os.linesep.join([f\"'{x}'\" for x in segmentation_files]),\n",
    "        \"[VOLUMES_FILE]\": volumes_file,\n",
    "    },\n",
    "    tempfile_name=volumes_name,\n",
    ")\n",
    "output = run_batch_file(volumes_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following distribution of intra-cranial volumes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read tissue volumes file\n",
    "\n",
    "import csv\n",
    "\n",
    "icvs = {}  # intra-cranial volumes per segmentation file\n",
    "\n",
    "\n",
    "def subject_id(segmentation_filename):\n",
    "    \"\"\"\n",
    "    Return subject id from segmentation file name\n",
    "    \"\"\"\n",
    "\n",
    "    sub_id = segmentation_filename.split(op.sep)[-4].replace(\"sub-\", \"\")\n",
    "    assert int(sub_id)\n",
    "    return sub_id\n",
    "\n",
    "\n",
    "with open(volumes_file) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        assert len(row) == 4, f\"Malformed row: {row}\"\n",
    "        icvs[subject_id(row[\"File\"])] = (\n",
    "            float(row[\"Volume1\"]) + float(row[\"Volume2\"]) + float(row[\"Volume3\"])\n",
    "        )\n",
    "\n",
    "plt.hist([icvs[x] for x in icvs])\n",
    "plt.xlabel(\"ICV (litres)\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** [ADD description here, include FWE threshold, contrasts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_batch = function(op.join(\"envs\", \"docker\", \"spm-batch.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grey matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job batch file written in stats_grey_matter__3913746513060334940_job.m\n",
      "Batch file written in stats_grey_matter__3913746513060334940_batch.m\n"
     ]
    }
   ],
   "source": [
    "# Stats batch (grey matter)\n",
    "stats_batch_job = op.join(\"code\", \"templates\", \"stats_job.m\")\n",
    "stats_name = preprocessing_name.replace(\"pre_processing\", \"stats\")\n",
    "\n",
    "design_dir = op.join(results_path, \"results-grey-matter\")\n",
    "os.makedirs(design_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def smwc_scan(tissue_class, patno, visit, protocol_description):\n",
    "    if not tissue_class in (1, 2):\n",
    "        raise Exception(f\"Unrecognized tissue class: {tissue_class}\")\n",
    "    expression = f\"{op.join(results_path, 'pre_processing')}/sub-{patno}/ses-{visit}/anat/smwc{tissue_class}PPMI*{utils.clean_protocol_description(protocol_description)}*.nii\"\n",
    "    files = glob.glob(expression)\n",
    "    assert (\n",
    "        len(files) == 1\n",
    "    ), f\"Zero or more than 1 files were matched by expression: {expression}\"\n",
    "    return op.abspath(files[0])\n",
    "\n",
    "\n",
    "# Don't mess up with ordering, it's critical\n",
    "rbds_smwc1 = [\n",
    "    f\"'{smwc_scan(1, patno, rbds[rbds['PATNO']==patno]['EVENT_ID'].values[0],rbds[rbds['PATNO']==patno]['Description'].values[0])},1'\"\n",
    "    for patno in sorted(rbds[\"PATNO\"])\n",
    "]\n",
    "controls_smwc1 = [\n",
    "    f\"'{smwc_scan(1, patno, controls[controls['PATNO']==patno]['EVENT_ID'].values[0], controls[controls['PATNO']==patno]['Description'].values[0])},1'\"\n",
    "    for patno in sorted(controls[\"PATNO\"])\n",
    "]\n",
    "\n",
    "groups_patnos = [x for group in (rbds, controls) for x in sorted(group[\"PATNO\"])]\n",
    "\n",
    "# Check orderding\n",
    "for i, x in enumerate(groups_patnos):\n",
    "    if i < len(rbds_smwc1):\n",
    "        assert f\"sub-{groups_patnos[i]}\" in rbds_smwc1[i]\n",
    "    else:\n",
    "        assert f\"sub-{groups_patnos[i]}\" in controls_smwc1[i - len(rbds_smwc1)]\n",
    "\n",
    "replace_keys = {\n",
    "    \"[DESIGN_DIR]\": op.abspath(design_dir),\n",
    "    \"[GROUP1_SMWC_SCANS]\": os.linesep.join(rbds_smwc1),\n",
    "    \"[GROUP2_SMWC_SCANS]\": os.linesep.join(controls_smwc1),\n",
    "    \"[ICVS]\": os.linesep.join(\n",
    "        [str(icvs[str(x)]) for x in groups_patnos]\n",
    "    ),  # don't mess up ordering\n",
    "    \"[AGES]\": os.linesep.join(\n",
    "        [\n",
    "            str(cohort[cohort[\"PATNO\"] == x][\"AGE_AT_VISIT\"].values[0])\n",
    "            for x in groups_patnos\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Stats batch\n",
    "stats_batch_job = op.join(\"code\", \"templates\", \"stats_job.m\")\n",
    "stats_name = preprocessing_name.replace(\"pre_processing\", \"stats_grey_matter\")\n",
    "\n",
    "write_batch_files(stats_batch_job, replace_keys, tempfile_name=stats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file stats_grey_matter__3913746513060334940.log does not exist, running batch\n",
      "Docker version 20.10.17, build 100c701\n",
      "Using default tag: latest\n",
      "Error response from daemon: manifest for glatard/spm-octave-5.2.0:latest not found: manifest unknown: manifest unknown\n",
      "Execution was successful.\n"
     ]
    }
   ],
   "source": [
    "output = run_batch_file(stats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check ordering of covariates once again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following significant clusters:\n",
    "\n",
    "**Contrast 1 (RBD-Controls)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>set.1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster.1</th>\n",
       "      <th>cluster.2</th>\n",
       "      <th>cluster.3</th>\n",
       "      <th>peak</th>\n",
       "      <th>peak.1</th>\n",
       "      <th>peak.2</th>\n",
       "      <th>peak.3</th>\n",
       "      <th>peak.4</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>equivk</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>T</td>\n",
       "      <td>equivZ</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>z {mm}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  set set.1      cluster    cluster.1 cluster.2 cluster.3         peak  \\\n",
       "0   p     c  p(FWE-corr)  p(FDR-corr)    equivk    p(unc)  p(FWE-corr)   \n",
       "\n",
       "        peak.1 peak.2  peak.3  peak.4 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0  p(FDR-corr)      T  equivZ  p(unc)           x           y      z {mm}  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_files = glob.glob(op.join(design_dir, \"spm_*_001.csv\"))\n",
    "assert (\n",
    "    len(results_files) == 1\n",
    "), f\"Expected exactly 1 result file, got {len(results_files)}\"\n",
    "df = pd.read_csv(results_files[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrast 2 (Controls-RBD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>set.1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster.1</th>\n",
       "      <th>cluster.2</th>\n",
       "      <th>cluster.3</th>\n",
       "      <th>peak</th>\n",
       "      <th>peak.1</th>\n",
       "      <th>peak.2</th>\n",
       "      <th>peak.3</th>\n",
       "      <th>peak.4</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>equivk</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>T</td>\n",
       "      <td>equivZ</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>z {mm}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  set set.1      cluster    cluster.1 cluster.2 cluster.3         peak  \\\n",
       "0   p     c  p(FWE-corr)  p(FDR-corr)    equivk    p(unc)  p(FWE-corr)   \n",
       "\n",
       "        peak.1 peak.2  peak.3  peak.4 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0  p(FDR-corr)      T  equivZ  p(unc)           x           y      z {mm}  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_files = glob.glob(op.join(design_dir, \"spm_*_002.csv\"))\n",
    "assert (\n",
    "    len(results_files) == 1\n",
    "), f\"Expected exactly 1 result file, got {len(results_files)}\"\n",
    "df = pd.read_csv(results_files[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job batch file written in stats_white_matter__3913746513060334940_job.m\n",
      "Batch file written in stats_white_matter__3913746513060334940_batch.m\n"
     ]
    }
   ],
   "source": [
    "design_dir = op.join(results_path, \"results-white-matter\")\n",
    "os.makedirs(design_dir, exist_ok=True)\n",
    "\n",
    "# Don't mess up with ordering, it's critical\n",
    "rbds_smwc2 = [\n",
    "    f\"'{smwc_scan(2, patno, rbds[rbds['PATNO']==patno]['EVENT_ID'].values[0],rbds[rbds['PATNO']==patno]['Description'].values[0])},1'\"\n",
    "    for patno in sorted(rbds[\"PATNO\"])\n",
    "]\n",
    "controls_smwc2 = [\n",
    "    f\"'{smwc_scan(2, patno, controls[controls['PATNO']==patno]['EVENT_ID'].values[0], controls[controls['PATNO']==patno]['Description'].values[0])},1'\"\n",
    "    for patno in sorted(controls[\"PATNO\"])\n",
    "]\n",
    "\n",
    "# Check orderding\n",
    "for i, x in enumerate(groups_patnos):\n",
    "    if i < len(rbds_smwc2):\n",
    "        assert f\"sub-{groups_patnos[i]}\" in rbds_smwc2[i]\n",
    "    else:\n",
    "        assert f\"sub-{groups_patnos[i]}\" in controls_smwc2[i - len(rbds_smwc2)]\n",
    "\n",
    "replace_keys = {\n",
    "    \"[DESIGN_DIR]\": op.abspath(design_dir),\n",
    "    \"[GROUP1_SMWC_SCANS]\": os.linesep.join(rbds_smwc2),\n",
    "    \"[GROUP2_SMWC_SCANS]\": os.linesep.join(controls_smwc2),\n",
    "    \"[ICVS]\": os.linesep.join(\n",
    "        [str(icvs[str(x)]) for x in groups_patnos]\n",
    "    ),  # don't mess up ordering\n",
    "    \"[AGES]\": os.linesep.join(\n",
    "        [\n",
    "            str(cohort[cohort[\"PATNO\"] == x][\"AGE_AT_VISIT\"].values[0])\n",
    "            for x in groups_patnos\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Stats batch\n",
    "stats_batch_job = op.join(\"code\", \"templates\", \"stats_job.m\")\n",
    "stats_name = preprocessing_name.replace(\"pre_processing\", \"stats_white_matter\")\n",
    "\n",
    "write_batch_files(stats_batch_job, replace_keys, tempfile_name=stats_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file stats_white_matter__3913746513060334940.log does not exist, running batch\n",
      "Docker version 20.10.17, build 100c701\n",
      "Using default tag: latest\n",
      "Error response from daemon: manifest for glatard/spm-octave-5.2.0:latest not found: manifest unknown: manifest unknown\n",
      "Execution was successful.\n"
     ]
    }
   ],
   "source": [
    "output = run_batch_file(stats_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the following significant clusters:\n",
    "\n",
    "**Contrast 1 (RBD-Controls)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>set.1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster.1</th>\n",
       "      <th>cluster.2</th>\n",
       "      <th>cluster.3</th>\n",
       "      <th>peak</th>\n",
       "      <th>peak.1</th>\n",
       "      <th>peak.2</th>\n",
       "      <th>peak.3</th>\n",
       "      <th>peak.4</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>equivk</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>T</td>\n",
       "      <td>equivZ</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>z {mm}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  set set.1      cluster    cluster.1 cluster.2 cluster.3         peak  \\\n",
       "0   p     c  p(FWE-corr)  p(FDR-corr)    equivk    p(unc)  p(FWE-corr)   \n",
       "\n",
       "        peak.1 peak.2  peak.3  peak.4 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0  p(FDR-corr)      T  equivZ  p(unc)           x           y      z {mm}  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_files = glob.glob(op.join(design_dir, \"spm_*_001.csv\"))\n",
    "assert (\n",
    "    len(results_files) == 1\n",
    "), f\"Expected exactly 1 result file, got {len(results_files)}\"\n",
    "df = pd.read_csv(results_files[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contrast 2 (Controls-RBD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>set</th>\n",
       "      <th>set.1</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster.1</th>\n",
       "      <th>cluster.2</th>\n",
       "      <th>cluster.3</th>\n",
       "      <th>peak</th>\n",
       "      <th>peak.1</th>\n",
       "      <th>peak.2</th>\n",
       "      <th>peak.3</th>\n",
       "      <th>peak.4</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>c</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>equivk</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>p(FWE-corr)</td>\n",
       "      <td>p(FDR-corr)</td>\n",
       "      <td>T</td>\n",
       "      <td>equivZ</td>\n",
       "      <td>p(unc)</td>\n",
       "      <td>x</td>\n",
       "      <td>y</td>\n",
       "      <td>z {mm}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  set set.1      cluster    cluster.1 cluster.2 cluster.3         peak  \\\n",
       "0   p     c  p(FWE-corr)  p(FDR-corr)    equivk    p(unc)  p(FWE-corr)   \n",
       "\n",
       "        peak.1 peak.2  peak.3  peak.4 Unnamed: 11 Unnamed: 12 Unnamed: 13  \n",
       "0  p(FDR-corr)      T  equivZ  p(unc)           x           y      z {mm}  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_files = glob.glob(op.join(design_dir, \"spm_*_002.csv\"))\n",
    "assert (\n",
    "    len(results_files) == 1\n",
    "), f\"Expected exactly 1 result file, got {len(results_files)}\"\n",
    "df = pd.read_csv(results_files[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dat.Dataset(datalad_path)\n",
    "# dat.push(dataset=d)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
